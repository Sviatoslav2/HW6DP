{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 6 - Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Data Preparation Course at UCU, 2019_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NB\n",
    "\n",
    "### __1) Which programming languages to use?__\n",
    "\n",
    "We recommend to use Python for this task, but if you find working library alternatives for the algorithms we\n",
    "use in this assignment in R, you are free to work with that as well.\n",
    "\n",
    "### __2) What libraries/packages to use?__\n",
    "\n",
    "You are free to choose any appropriate libraries (good choice would be __pandas__, __numpy__,\n",
    "__scicit-learn__).\n",
    "\n",
    "### __3) How to summarize my homework?__\n",
    "\n",
    "The best way is to create an Jupyter/R notebook with code and explanations for each strategy. In case you\n",
    "are not familiar with these tools, you can create a Python/R scripts and write explanations as comments.\n",
    "However, we strongly recommend you to use Jupyter/R notebooks, as those are #1 tools in applied data\n",
    "analysis nowadays.\n",
    "\n",
    "__Please do not include large datasets in the archive with your notebook(s)!__\n",
    "\n",
    "### __4) Useful links__\n",
    "\n",
    "1. [Q/A on Dimentionality Reduction Techniques](https://www.analyticsvidhya.com/blog/2017/03/questions-dimensionality-reduction-data-scientist/)\n",
    "2. [The Ultimate Guide to 12 Dimensionality Reduction Techniques](https://www.analyticsvidhya.com/blog/2018/08/dimensionality-reduction-techniques-python/)\n",
    "3. [Reducing Dimentionality](https://towardsdatascience.com/reducing-dimensionality-from-dimensionality-reduction-techniques-f658aec24dfe)\n",
    "4. [PCA in Details](https://towardsdatascience.com/dimensionality-reduction-does-pca-really-improve-classification-outcome-6e9ba21f0a32)\n",
    "5. [Why, How and When to apply Feature Selection](https://towardsdatascience.com/why-how-and-when-to-apply-feature-selection-e9c69adfabf2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "### __1) [5pt] Feature selection__\n",
    "\n",
    "__1.1.__ Download the Spambase Data Set from UCI ML repository __[(Dataset)](http://archive.ics.uci.edu/ml/datasets/Spambase)__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOU CAN PLACE YOUR SOLUTION IN THE CELLS LIKE THIS ##\n",
    "#######################################################\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as sfs\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "from matplotlib import pyplot\n",
    "#######################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1.2.__ Train a regular Logistic Regression model using original attributes. Save its accuracy on `train` and `test`\n",
    "sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################################################################################\n",
      "Accuracy score on test :  0.9216589861751152\n",
      "f1_score on test :  0.9053301511535402\n",
      "Accuracy score on train :  0.9198572355613238\n",
      "f1_score on train :  0.8939458995276943\n",
      "#####################################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fedoriv\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False), 0.9216589861751152)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('spambase.data', header=None, usecols=[i for i in range(58)] )\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.drop(57,axis=1), data[57], test_size=0.33, random_state=42)\n",
    "\n",
    "model = LogisticRegression(solver='lbfgs')\n",
    "def model_fit(model, X_train, X_test, y_train, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_predict_test = model.predict(X_test) \n",
    "    \n",
    "    y_predict_train = model.predict(X_train) \n",
    "    \n",
    "    \n",
    "    print(\"#####################################################################################\")\n",
    "    print(\"Accuracy score on test : \",accuracy_score(y_test.values, y_predict_test))\n",
    "    print(\"f1_score on test : \",f1_score(y_test, y_predict_test))\n",
    "    print(\"Accuracy score on train : \",accuracy_score(y_train.values, y_predict_train))\n",
    "    print(\"f1_score on train : \",f1_score(y_train, y_predict_train))\n",
    "    print(\"#####################################################################################\")\n",
    "    return model, accuracy_score(y_test, y_predict_test) \n",
    "model_fit(model, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1.3.__ Apply Forward Stepwise Selection technique to find the subset of attributes which minimizes an estimate of the expected predition error. Visualize the process of this selection (subset size on $x$-axis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_clf = RandomForestClassifier(n_estimators=100, n_jobs=-1)\n",
    "\n",
    "def get_sfs_list(model, X_train, y_train, forward=True):\n",
    "    sfs1 = sfs(model,\n",
    "           k_features=5,\n",
    "           forward=forward,\n",
    "           floating=False,\n",
    "           verbose=2,\n",
    "           scoring='accuracy',\n",
    "           cv=5)\n",
    "    sfs1 = sfs1.fit(X_train, y_train)\n",
    "    return list(sfs1.k_feature_idx_)\n",
    "#get_sfs_list(model_clf, X_train, y_train) # [6,15,51,52,56]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1.3.1.__ Train a Logistion Regression model using the features found with FSS. Save its accuracy on `train` and `test` datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################################################################################\n",
      "Accuracy score on test :  0.8426596445029625\n",
      "f1_score on test :  0.7856502242152466\n",
      "Accuracy score on train :  0.8465282284231019\n",
      "f1_score on train :  0.7718282682103232\n",
      "#####################################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fedoriv\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False), 0.8426596445029625)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_ = X_train.iloc[:,[6,15,51,52,56]]\n",
    "X_test_ = X_test.iloc[:,[6,15,51,52,56]]\n",
    "\n",
    "\n",
    "model_fit(model, X_train_, X_test_, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1.4.__ Apply Backward Stepwise Selection technique to find the subset of attributes which minimizes an estimate of the expected predition error. Visualize the process of this selection (subset size on $x$-axis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_sfs_list(model, X_train, y_train, False) [6,15,51,52,56]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1.4.1.__ Train a Logistion Regression model using the features found with BSS. Save its accuracy on `train` and `test` datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################################################################################\n",
      "Accuracy score on test :  0.8426596445029625\n",
      "f1_score on test :  0.7856502242152466\n",
      "Accuracy score on train :  0.8465282284231019\n",
      "f1_score on train :  0.7718282682103232\n",
      "#####################################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fedoriv\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False), 0.8426596445029625)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_ = X_train.iloc[:,[6,15,51,52,56]]\n",
    "X_test_ = X_test.iloc[:,[6,15,51,52,56]]\n",
    "model_fit(model, X_train_, X_test_, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1.5.__ Compare two approaches (FSS and BSS). Did they found the same subset? If not, explain why it could have happend?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1.6.__ Use Decision Trees to find important features. Visualize relative feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00000000e+00 5.40649518e-04 4.99617588e-03 0.00000000e+00\n",
      " 2.84379385e-02 4.31605382e-03 9.97658372e-02 6.61757914e-03\n",
      " 2.79248459e-04 0.00000000e+00 4.18057153e-03 3.63323418e-03\n",
      " 0.00000000e+00 7.16120424e-03 4.32696193e-03 3.50644104e-02\n",
      " 1.55217545e-02 3.26744001e-03 4.62176464e-03 6.10240549e-03\n",
      " 4.39545996e-02 7.22848205e-03 1.31867249e-02 2.39387117e-02\n",
      " 3.93231809e-02 8.04676861e-03 2.95155924e-02 2.06668638e-02\n",
      " 5.32067008e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 6.04390400e-03\n",
      " 1.66909043e-02 0.00000000e+00 5.41894464e-03 0.00000000e+00\n",
      " 4.04551113e-03 1.28080165e-02 3.63075081e-03 7.19894888e-03\n",
      " 8.48191325e-03 2.25484557e-02 0.00000000e+00 6.34154864e-03\n",
      " 4.71616583e-03 3.26999486e-03 0.00000000e+00 1.07767612e-01\n",
      " 2.92129815e-01 6.03038003e-04 4.91107777e-02 1.99757125e-02\n",
      " 9.20316018e-03]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "print(model.feature_importances_)\n",
    "# plot\n",
    "pyplot.bar(range(len(model.feature_importances_)), model.feature_importances_)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1.6.1.__ Train a Logistic Regression model using the features found with DT. Save its accuracy on `train` and `test` datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################################################################################\n",
      "Accuracy score on test :  0.9005924950625411\n",
      "f1_score on test :  0.8740617180984154\n",
      "Accuracy score on train :  0.9231018818948734\n",
      "f1_score on train :  0.8958241758241758\n",
      "#####################################################################################\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "               colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "               learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "               min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
       "               nthread=None, objective='binary:logistic', random_state=0,\n",
       "               reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "               silent=None, subsample=1, verbosity=1), 0.9005924950625411)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_features(model):\n",
    "    x = model.feature_importances_.tolist()\n",
    "    lst = []\n",
    "    for i in range(5):\n",
    "        lst.append(max(x))\n",
    "        x.remove(max(x))\n",
    "    indexes =[]\n",
    "    for item in lst:\n",
    "        indexes.append(model.feature_importances_.tolist().index(item))\n",
    "    return indexes\n",
    "lst = get_features(model)\n",
    "\n",
    "\n",
    "X_train_ = X_train.iloc[:,lst]\n",
    "X_test_ = X_test.iloc[:,lst]\n",
    "\n",
    "\n",
    "model_fit(model, X_train_, X_test_, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1.7.__ Apply the following “manual” techniques to detect redundant features: missing value ratio, low variance and high correlation filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features after removing redundant features:  5\n",
      "number of features after removing redundant features:  5\n"
     ]
    }
   ],
   "source": [
    "x = model.feature_importances_.tolist()\n",
    "indexesOfFeatures = []\n",
    "for i in x:\n",
    "    if(i != 0):\n",
    "        indexesOfFeatures.append(x.index(i))\n",
    "print('number of features after removing redundant features: ',len(indexesOfFeatures))\n",
    "print('number of features after removing redundant features: ',len(model.feature_importances_.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1.7.1.__ Remove features you found (in 2.7.) from the dataset and train a Logistic Regression model using the rest of the data. Save its accuracy on `train` and `test` datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1.8.__ Compare the results of different methods of feature selection. Write pros/cons of each of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\#\\# You can place your answer here \\#\\#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
